# GreenPrompt ðŸŒ±

## Overview
GreenPrompt is a local-first tool that tracks and visualizes the real-world energy consumption of AI prompts, locally. It intercepts or wraps LLM calls (via Ollamaâ€™s local API), samples CPU/GPU power, logs prompt metadata and energy usage, and provides both CLI and Web UI interfaces for analysis.

> **macOS only** (uses `powermetrics` for power sampling)

## Features
- **Baseline & Prompt Sampling**: Continuously samples system power in a 10â€‘minute sliding window; computes a 1â€‘minute baseline before each prompt and measures power during execution.
- **Token & Model Tracking**: Captures prompt and completion token counts, model used.
- **Energy Estimation**: Calculates energy in Wh based on sampled power data and execution duration.
- **Persistent Logging**: Stores every prompt run in a SQLite database (`greenprompt_usage.db`).
- **CLI Interface**: Run prompts, view immediate stats.
- **Web API & Dashboard**: Flask-based endpoints for retrieving usage and a simple HTML dashboard.
- **Proxy Server**: (Optional) Intercept existing Ollama API traffic to log usage without code changes.

## Architecture & Flow
1. **Power Sampling**  
   - `samplerMac.py` defines `PowerMonitor`, a background thread that samples combined CPU/GPU power every second and retains a 10â€‘minute history.
2. **Prompt Execution**  
   - `core.py`â€™s `run_prompt` function:
     - Retrieves a 1â€‘minute baseline average from `PowerMonitor`.
     - Sends the user prompt to Ollama at `localhost:11434` (or via proxy).
     - Records start and end timestamps.
     - Queries `PowerMonitor` for samples during execution.
     - Calculates average power and energy.
3. **Data Persistence**  
   - `dbconn.py` provides `init_db()` and `save_prompt_usage()`, storing each runâ€™s metadata in SQLite.
4. **Interfaces**  
   - **CLI** (`cli.py`): `greenprompt <prompt>` to run and log.
   - **Web App** (`web.py`/`api.py`):  
     - `POST /api/generate` to run prompts.  
     - `GET /api/usage/all` / `model/<model>` / `timeframe` to retrieve logs.
   - **Setup Script** (`setup.py`): Initializes the DB and rebinds Ollama port.

## File Structure
```
greenprompt/
â”œâ”€â”€ cli.py                  # CLI entry point
â”œâ”€â”€ core.py                 # Prompt runner & energy estimator
â”œâ”€â”€ dbconn.py               # SQLite connection, init, and save/query functions
â”œâ”€â”€ samplerMac.py           # macOS PowerMonitor for sampling power
â”œâ”€â”€ sysUsage.py             # OS-agnostic wrappers & estimators
â”œâ”€â”€ setup.py                # Tool setup: DB init, Ollama rebind
â”œâ”€â”€ proxy.py                # Optional HTTP proxy for Ollama API
â”œâ”€â”€ web.py                  # Flask web server, HTML dashboard
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html          # Web UI template
â”œâ”€â”€ static/                 # Optional assets (CSS)
â””â”€â”€ constants.py            # User-editable settings
```

## Installation & Setup
1. **Requirements**  
   - macOS with `powermetrics` (requires `sudo` privileges)  
   - Python 3.8+  
   - Ollama installed and models pulled

2. **Clone & Install**  
   ```bash
   git clone <repo-url>
   cd greenprompt
   poetry install
   ```

3. **Initialize**  
   ```bash
   python setup.py
   ```
   - Creates `greenprompt_usage.db`  
   - Rebinds Ollama to port 11435 (via setup instructions)

4. **Start Sampling Daemon** (optionally in a TMUX or background)  
   ```bash
   python samplerMac.py
   ```

## Usage

### CLI
```bash
greenprompt --setup            # Setup Ollama port & DB
greenprompt "Hello, world!"    # Run prompt via CLI and log
greenprompt --model llama2     # Specify model
```

### Web Dashboard
```bash
python web.py
# Open http://localhost:5000 in your browser
```

### API Endpoints
- `POST /api/generate` â†’ Run a prompt  
- `GET /api/usage/all` â†’ All records  
- `GET /api/usage/model/<model>` â†’ Records for a model  
- `GET /api/usage/timeframe?start=ISO&end=ISO` â†’ Timeâ€‘filtered records  

## Limitations
- **macOS only** (relies on `powermetrics`)  
- Per-process power usage is estimated proportionally from system metrics  
- CLI proxy requires redirecting Ollama to use `localhost:11434`

## Future Work
- Linux & Windows support (via Intel Power Gadget, `rapl`, `nvidia-smi`)  
- VS Code & browser extensions  
- Carbon offset integrations  
- Team dashboards & enterprise reporting

## License
[MIT License](LICENSE)